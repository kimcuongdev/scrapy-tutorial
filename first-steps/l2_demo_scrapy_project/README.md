# üìÇ C·∫•u tr√∫c th∆∞ m·ª•c project c·ªßa Scrapy
```
tutorial/
    scrapy.cfg            # file c·∫•u h√¨nh cho Scrapy khi ch·∫°y project

    tutorial/             # module Python c·ªßa project
        __init__.py

        items.py          # m√¥ t·∫£ c·∫•u tr√∫c d·ªØ li·ªáu tr√≠ch xu·∫•t ra

        middlewares.py    # ki·ªÉm so√°t request/response

        pipelines.py      # x·ª≠ l√Ω d·ªØ li·ªáu ƒë∆∞·ª£c yield ra t·ª´ spiders

        settings.py       # c·∫•u h√¨nh project

        spiders/          # ch·ª©a c√°c spider
            __init__.py
```

# ‚ñ∂Ô∏è Ch·∫°y spider trong Scrapy project
ƒê·ªÉ ch·∫°y spider trong project, ƒë·∫£m b·∫£o b·∫°n ƒëang ·ªü th∆∞ m·ª•c project, ch·∫°y l·ªánh:
```bash
scrapy crawl <spider_id>
```

# üìë Tr√≠ch xu·∫•t d·ªØ li·ªáu trong Scrapy

## 1. S·ª≠ d·ª•ng CSS Selector
**C√∫ ph√°p c∆° b·∫£n:**
```python
response.css("CSS_SELECTOR")
```

- K·∫øt qu·∫£ tr·∫£ v·ªÅ l√† object `SelectorList` (danh s√°ch c√°c node HTML t√¨m ƒë∆∞·ª£c).
- Ti·∫øp t·ª•c b·∫±ng c√°ch l·∫•y m·ªôt k·∫øt qu·∫£ b·∫±ng `.get()` ho·∫∑c l·∫•y t·∫•t c·∫£ k·∫øt qu·∫£ b·∫±ng `.getall()`.

---

### 1.1. V√≠ d·ª• minh ho·∫°
Gi·∫£ s·ª≠ HTML c√≥ d·∫°ng:
```html
<div class="quote">
  <span class="text">‚ÄúThe world as we have created it...‚Äù</span>
  <span>by <small class="author">Albert Einstein</small></span>
</div>
```

#### L·∫•y tr√≠ch d·∫´n
```python
response.css("div.quote span.text::text").get()
```

#### L·∫•y t√™n t√°c gi·∫£
```python
response.css("div.quote small.author::text").get()
```

**Output:**
```python
>>> response.css("div.quote small.author::text").get()
'Albert Einstein'
```

---

### 1.2 C√°c h·∫≠u t·ªë th∆∞·ªùng d√πng
- **`::text`**: ch·ªâ l·∫•y ph·∫ßn text trong tag.
```python
>>> response.css("div.quote small.author").get()
'<small class="author" itemprop="author">Albert Einstein</small>'
```

- **`::attr(href)`**: l·∫•y gi√° tr·ªã thu·ªôc t√≠nh.
### 1.3. Ngo·∫°i l·ªá
Truy c·∫≠p v√†o `SelectorList` r·ªóng s·∫Ω cho ra ngo·∫°i l·ªá `IndexError`:
```
>>>response.css("noelement")[0].get()
Traceback (most recent call last):
...
IndexError: list index out of range
```
Thay v√†o ƒë√≥, n√™n d√πng `get()` v√¨ k·∫øt qu·∫£ tr·∫£ v·ªÅ s·∫Ω l√† `None` n·∫øu r·ªóng
```
response.css("noelement").get()
```
## 2. S·ª≠ d·ª•ng XPath
### 2.1. C√∫ ph√°p c∆° b·∫£n
```python
response.xpath("XPATH_EXPRESSION")
```
* Tr·∫£ v·ªÅ `SelectorList` gi·ªëng `.css()`
* D√πng `get()` ƒë·ªÉ l·∫•y m·ªôt k·∫øt qu·∫£ ho·∫∑c `getall()` ƒë·ªÉ l·∫•y h·∫øt
### 2.2 V√≠ d·ª• minh ho·∫°
* Gi·∫£ s·ª≠ HTML c√≥ d·∫°ng
```html
<div class="quote">
  <span class="text">‚ÄúThe world as we have created it...‚Äù</span>
  <span>by <small class="author">Albert Einstein</small></span>
</div>
```
* L·∫•y tr√≠ch d·∫´n
  * `//div[@class='quote]`: t√¨m t·∫•t c·∫£ th·∫ª `div` c√≥ `class` l√† `quote` trong HTML
  * `/span[@class= 'text']`: ƒëi xu·ªëng m·ª©c con, t√¨m c√°c th·∫ª `span` c√≥ `class` l√† `text`
  * `/text()`: l·∫•y text b√™n trong
```python
response.xpath("//div[@class='quote']/span[@class='text']/text()").get()
```
* L·∫•y t√°c gi·∫£
  * `//small[@class='author']` ƒë·ªÉ ƒëi xu·ªëng b·∫•t k·ª≥ m·ª©c n√†o, t√¨m th·∫ª `small`
```python
response.xpath("//div[@class= 'quote]//small[@class='author']/text()").get()
```
## 3. Tr√≠ch xu·∫•t trong spider
### T·ª´ kho√° `yield`
* `yield` bi·∫øn m·ªôt h√†m b√¨nh th∆∞·ªùng th√†nh *generation function*
* Khi h√†m ch·∫°y ƒë·∫øn `yield`, n√≥ tr·∫£ v·ªÅ gi√° tr·ªã ƒë√≥ nh∆∞ng *kh√¥ng k·∫øt th√∫c h√†m*
* L·∫ßn g·ªçi h√†m ti·∫øp theo s·∫Ω ti·∫øp t·ª•c ch·∫°y t·ª´ ch·ªó `yield` tr·ªü ƒëi
```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        "https://quotes.toscrape.com/page/1/",
        "https://quotes.toscrape.com/page/2/",
    ]

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                "text": quote.css("span.text::text").get(),
                "author": quote.css("small.author::text").get(),
                "tags": quote.css("div.tags a.tag::text").getall(),
            }
```
# L∆∞u tr·ªØ d·ªØ li·ªáu crawl ƒë∆∞·ª£c
## Feed Exports
* Option `-O` ƒë·ªÉ ghi ƒë√® n·∫øu file ƒë√£ t·ªìn t·∫°i
* Option `-o` thay v√†o ƒë√≥ g√°n d·ªØ li·ªáu m·ªõi v√†o file ƒë√£ t·ªìn t·∫°i
```bash
scrapy crawl quotes -O quotes.json
```
# T√¨m ƒë·∫øn url k·∫ø ti·∫øp
* T√¨m ƒë·∫øn m·ªôt url
```python
        next_page = response.css("li.next a::attr(href)").get()
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)
```
* T√¨m ƒë·∫øn t·∫•t c·∫£ c√°c url
  * `author_page_links = response.css(".author + a")`
    * T√¨m ƒë·∫øn t·∫•t c·∫£ c√°c link n·∫±m ngay sau t√™n c√°c t√°c gi·∫£, ch√≠nh l√† link bio c·ªßa t√°c gi·∫£
      * `response.css(".author + a")`: Th·∫ª `<a>` ƒë·ª©ng ngay sau ph·∫ßn t·ª≠ c√≥ class l√† `author`
  * `yield from response.follow_all(author_page_links, self.parse_author)`
    * V·ªõi m·ªói link t√¨m ƒë∆∞·ª£c, Scrapy s·∫Ω follow theo ƒë·ªÉ ƒë·∫øn bio c·ªßa t√°c gi·∫£ v√† g·ªçi h√†m `parse_author` ƒë·ªÉ x·ª≠ l√Ω.
  * `pagination_links = response.css("li.next a")`
    * L·∫•y link ƒë·∫øn trang k·∫ø ti·∫øp
  * `yield from response.follow_all(pagination_links, self.parse)`
    * ƒêi ƒë·∫øn trang k·∫ø ti·∫øp v√† g·ªçi h√†m `parse` ƒë·ªÉ x·ª≠ l√Ω
```python
def parse(self, response):
    author_page_links = response.css(".author + a")
    yield from response.follow_all(author_page_links, self.parse_author)

    pagination_links = response.css("li.next a")
    yield from response.follow_all(pagination_links, self.parse)

def parse_author(self, response):
    def extract_with_css(query):
        return response.css(query).get(default="").strip()

    yield {
        "name": extract_with_css("h3.author-title::text"),
        "birthdate": extract_with_css(".author-born-date::text"),
        "bio": extract_with_css(".author-description::text"),
    }
```
# Truy·ªÅn tham s·ªë khi g·ªçi spider
```
scrapy crawl quotes -O quotes-humor.json -a tag=humor
```
* Truy·ªÅn tham s·ªë s·ª≠ d·ª•ng option `-a`
```
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"

    async def start(self):
        url = "https://quotes.toscrape.com/"
        tag = getattr(self, "tag", None)
        if tag is not None:
            url = url + "tag/" + tag
        yield scrapy.Request(url, self.parse)

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                "text": quote.css("span.text::text").get(),
                "author": quote.css("small.author::text").get(),
            }

        next_page = response.css("li.next a::attr(href)").get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
```
